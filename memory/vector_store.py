"""Persistent vector memory store built on top of ChromaDB."""

from __future__ import annotations

import os
import uuid
from typing import Dict, List, Optional

import chromadb
from chromadb.utils.embedding_functions import EmbeddingFunction

from agent.local_llm import get_embeddings


class _LocalEmbeddingFunction(EmbeddingFunction):
    """Adapter so Chroma can call our embedding helper directly."""

    def __call__(self, texts: List[str]) -> List[List[float]]:
        return [get_embeddings(text) for text in texts]


class MemoryStore:
    """Manage conversational memories with persistent ChromaDB storage."""

    def __init__(
        self,
        persist_directory: str = "./data/memory_store",
        collection_name: str = "codex_memory",
    ) -> None:
        """
        Initialize the vector store and embedding model.

        Parameters
        ----------
        persist_directory:
            Location where ChromaDB will persist the collection on disk.
        collection_name:
            Name of the vector collection storing conversation snippets.
        """
        os.makedirs(persist_directory, exist_ok=True)
        self._client = chromadb.PersistentClient(path=persist_directory)
        self._collection = self._client.get_or_create_collection(
            name=collection_name,
            embedding_function=_LocalEmbeddingFunction(),
        )

    def add_memory(
        self,
        text: str,
        metadata: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        Persist a single conversational memory entry.

        Returns
        -------
        memory_id:
            Identifier generated by Chroma for the persisted memory.
        """
        memory_id = str(uuid.uuid4())
        self._collection.add(
            ids=[memory_id],
            documents=[text],
            metadatas=[metadata or {}],
        )
        return memory_id

    def search(self, query: str, k: int = 4) -> List[str]:
        """
        Retrieve the most relevant memories for a query.

        Parameters
        ----------
        query:
            The natural language prompt used to surface prior context.
        k:
            Maximum number of memories to return.
        """
        query_embedding = get_embeddings(query)
        result = self._collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
        )
        documents = result.get("documents") or []
        return documents[0] if documents else []
